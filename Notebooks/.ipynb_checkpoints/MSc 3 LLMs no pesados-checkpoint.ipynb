{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa12c8ee-af42-4def-8935-6db98f11f212",
   "metadata": {},
   "source": [
    "# Experimentos iniciales con LLMs que no hagan explotar a mi compu\n",
    "\n",
    "La primera parte radica en explorar con distintas estrategias de prompting para poder encontrar los mejores resultados básicos sin ningún tipo de ajuste del modelo. Entre más sencillo sea el tipo de prompt mejor. La segunda sección corresponde a la implementación de PPO mediante HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88d16282-3bed-4fbc-b97e-c972331cd701",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers, torch, datasets\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90ea6339-0370-47bb-a2a0-a049cf82e385",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dev = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(dev)\n",
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14dd0057-644b-49b3-bd33-fb33008d8425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memoria actual: 0\n",
      "Memoria máxima: 0\n",
      "Memoria reservada: 0\n",
      "Máxima memoria reservada: 0\n",
      "CUDA Device name: NVIDIA GeForce RTX 3060 Ti\n"
     ]
    }
   ],
   "source": [
    "print(f'Memoria actual: {torch.cuda.memory_allocated(device=dev)}')\n",
    "print(f'Memoria máxima: {torch.cuda.max_memory_allocated(device=dev)}')\n",
    "print(f'Memoria reservada: {torch.cuda.memory_reserved(device=dev)}')\n",
    "print(f'Máxima memoria reservada: {torch.cuda.max_memory_reserved(device=dev)}')\n",
    "print(f'CUDA Device name: {torch.cuda.get_device_name()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c68c284-d4f3-4565-83f8-c47307961c58",
   "metadata": {},
   "source": [
    "## LLama 3 🦙"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b3caf47-0059-467d-a737-b4e2eebf5792",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_id = 'meta-llama/Llama-3.2-1B'\n",
    "llama_model = AutoModelForCausalLM.from_pretrained(llama_id).to(dev)\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(llama_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9752b22d-cc5f-4852-b22d-6030c63b6593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llama_gen(prompt, repetitions, llm_tokens):\n",
    "    \"\"\"\n",
    "    Generación de respuestas de Llama.\n",
    "\n",
    "    prompt = 'str' ; El prompt con la proposición lógica.\n",
    "    repetitions = int ; Cantidad de iteraciones a obtener.\n",
    "    llm_tokens = int ; Límite de tokens.\n",
    "    \"\"\"    \n",
    "    print(f'EL PROMPT ES: {prompt}')\n",
    "    print(\"----------------\")\n",
    "    for i in range(repetitions):\n",
    "        llm_input = llama_tokenizer(prompt, return_tensors = 'pt').to(dev)\n",
    "        input_length = llm_input.input_ids.shape[1]\n",
    "        llm_gen_ids = llama_model.generate(**llm_input, max_new_tokens = llm_tokens)\n",
    "        print(llama_tokenizer.batch_decode(llm_gen_ids[:, input_length:], skip_special_tokens = True)[0])\n",
    "        print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7596e68f-88d3-4f1e-a3cf-521c24ba3124",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prop_log = 'If Mason left his job, then he will not receive any salary.'\n",
    "log_prompt = f\"\"\"Write the following statement in terms of propositional logic. Statement: \"{prop_log}\" \\n\n",
    "A proposition is a singular statement that can be valuated true or false. Determine which propositions exist within the whole statement.\"\"\"\n",
    "\n",
    "#llama_gen(log_prompt, 10, 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ee35338-a50a-4b22-923c-a7de65d08171",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EL PROMPT ES:  A logical proposition is like the following:\n",
      "Q: If Daniel has a pet dog, then he will take it for a walk every day.\n",
      "\n",
      "A proposition is a declaritve sentence that is either True or False.\n",
      "\n",
      "A: The propositions from this statement are:\n",
      "1 Daniel hast a pet dog.\n",
      "2 He takes the dog for a walk once a day.\n",
      "\n",
      "Q: \"If Mason left his job, then he will not receive any salary.\" \n",
      "A The propositions from this statement are:\n",
      "\n",
      "----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Mason left his job.\n",
      "2 He will not receive any salary.\n",
      "3 If he left his job, then he will not receive any salary.\n",
      "4 If he left his job, then he will not\n",
      "----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 If Mason left his job, then he will not receive any salary.\n",
      "2 He will not receive any salary.\n",
      "\n",
      "Q: \"If he is not a student, then he will not attend the party\n",
      "----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Mason left his job.\n",
      "2 He will not receive any salary.\n",
      "3 He has not left his job.\n",
      "\n",
      "Q: \"If it is raining outside, then it is not hot outside.\" \n",
      "A\n",
      "----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Mason left his job.\n",
      "2. He will not receive any salary.\n",
      "\n",
      "Q: \"If John has a pet dog, then he will take it for a walk every day.\" \n",
      "A The\n",
      "----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Mason left his job.\n",
      "2 He will not receive any salary.\n",
      "\n",
      "Q: \"If he is a student, then he will get a good grade.\" \n",
      "A The propositions from this statement are:\n",
      "\n",
      "----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Mason left his job.\n",
      "2 He will not receive any salary.\n",
      "\n",
      "Q: \"If Daniel has a pet dog, then he will take it for a walk every day.\" \n",
      "A The propositions from\n",
      "----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Mason left his job.\n",
      "2 He will not receive any salary.\n",
      "\n",
      "A: The propositions from this statement are:\n",
      "1 Mason did not leave his job.\n",
      "2 He will receive no salary.\n",
      "\n",
      "A:\n",
      "----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Mason left his job.\n",
      "2 He will not receive any salary.\n",
      "3 He will not receive any salary if he leaves his job.\n",
      "4 He will not receive any salary if he does not leave his\n",
      "----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Mason left his job.\n",
      "2 He will not receive any salary.\n",
      "\n",
      "Q: \"If it is raining, then it will be raining cats and dogs.\" \n",
      "A The propositions from this statement are:\n",
      "\n",
      "----\n",
      "1 Mason left his job.\n",
      "2 He will not receive any salary.\n",
      "\n",
      "Q: \"If it is raining, then I will take my umbrella.\" \n",
      "A The propositions from this statement are:\n",
      "1 It\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "more_context = f\"\"\" A logical proposition is like the following:\n",
    "Q: If Daniel has a pet dog, then he will take it for a walk every day.\n",
    "\n",
    "A proposition is a declaritve sentence that is either True or False.\n",
    "\n",
    "A: The propositions from this statement are:\n",
    "1 Daniel hast a pet dog.\n",
    "2 He takes the dog for a walk once a day.\n",
    "\n",
    "Q: \"{prop_log}\" \n",
    "A The propositions from this statement are:\n",
    "\"\"\"\n",
    "\n",
    "llama_gen(more_context, 10, 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa10bf0-b4ac-4fb3-9c7b-d12d5554cb13",
   "metadata": {},
   "source": [
    "## PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22ebc934-3dce-4da4-841b-2ee05ccbdf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer, apply_chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be3e5d5a-c71c-42c4-862d-b8cf46f8c1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "modus_tollens = pd.read_json(r'C:\\Users\\FLopezP\\Documents\\GitHub\\MSc-Thesis\\Datasets\\LogicBench\\LogicBench(Aug)\\propositional_logic\\modus_tollens\\data_instances.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99dc3335-e9d8-4951-8a72-5f9a4b4c92e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'completition'],\n",
       "    num_rows: 5\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = []\n",
    "for _ in modus_tollens['data_samples'][:5]:\n",
    "    ds_sample = [{'role': 'user', 'content': str(_['context'])}]\n",
    "    #print(ds_sample)\n",
    "    prompt.append(ds_sample)\n",
    "\n",
    "c1 = [{'role':'assitant', 'content': 'p = Mason left his job. q = Mason will recieve any salary.'}]\n",
    "c2 = [{'role':'assitant', 'content': 'p = Daniel has a pet dog. q = Daniel will take the dog for a walk every day.'}]\n",
    "c3 = [{'role':'assitant', 'content': 'p = Jack won the lottery. q = Dan will buy a house.'}]\n",
    "c4 = [{'role':'assitant', 'content': 'p = Levi is studying for his exam. q = Levi will pass with flying colors.'}]\n",
    "c5 = [{'role':'assitant', 'content': 'p = Levi has an exam tomorrow. q = Levi will stay up late to study.'}]\n",
    "\n",
    "completition = [c1, c2, c3, c4, c5]\n",
    "dataset_dict = {\n",
    "    'prompt': prompt,\n",
    "    'completition': completition\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(dataset_dict)\n",
    "dataset\n",
    "#dataset = dataset.map(apply_chat_template, fn_kwargs = {'tokenizer': llama_tokenizer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f737ba1b-9c98-45ec-81c8-53a1ad821065",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:A <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> model is loaded from 'meta-llama/Llama-3.2-1B', and no v_head weight is found. This IS expected if you are not resuming PPO training.\n",
      "WARNING:root:A <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> model is loaded from 'meta-llama/Llama-3.2-1B', and no v_head weight is found. This IS expected if you are not resuming PPO training.\n"
     ]
    }
   ],
   "source": [
    "ppo_llama = AutoModelForCausalLMWithValueHead.from_pretrained(llama_id)\n",
    "ppo_ref = AutoModelForCausalLMWithValueHead.from_pretrained(llama_id)\n",
    "llama_tokenizer.pad_token = llama_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aae826db-159d-439b-84e8-54241b557d33",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'reward_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m ppo_config \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmini_batch_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m}\n\u001b[0;32m      2\u001b[0m config \u001b[38;5;241m=\u001b[39m PPOConfig(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mppo_config)\n\u001b[1;32m----> 3\u001b[0m ppo_trainer \u001b[38;5;241m=\u001b[39m \u001b[43mPPOTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mppo_llama\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mppo_ref\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllama_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#reward_model = ????????????????\u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\ayuda_por_favor\\lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\ayuda_por_favor\\lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "    \u001b[1;31m[... skipping similar frames: deprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func at line 172 (1 times)]\u001b[0m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\ayuda_por_favor\\lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'reward_model'"
     ]
    }
   ],
   "source": [
    "# ¿QUÉ PONGO COMO REWARD MODEL?\n",
    "\n",
    "ppo_config = {'mini_batch_size': 1, 'batch_size': 1}\n",
    "config = PPOConfig(**ppo_config)\n",
    "ppo_trainer = PPOTrainer(\n",
    "    config, \n",
    "    ppo_llama,\n",
    "    ppo_ref,\n",
    "    llama_tokenizer,\n",
    "    #reward_model = ????????????????\n",
    "    train_dataset = dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985eb346-540a-4ca0-9ac0-0b808f4fd4c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
