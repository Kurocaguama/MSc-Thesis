{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa12c8ee-af42-4def-8935-6db98f11f212",
   "metadata": {},
   "source": [
    "# Experimentos iniciales con LLMs que no hagan explotar a mi compu\n",
    "\n",
    "La primera parte radica en explorar con distintas estrategias de prompting para poder encontrar los mejores resultados b치sicos sin ning칰n tipo de ajuste del modelo. Entre m치s sencillo sea el tipo de prompt mejor. La segunda secci칩n corresponde a la implementaci칩n de PPO mediante HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88d16282-3bed-4fbc-b97e-c972331cd701",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers, torch, datasets\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90ea6339-0370-47bb-a2a0-a049cf82e385",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dev = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(dev)\n",
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14dd0057-644b-49b3-bd33-fb33008d8425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memoria actual: 0\n",
      "Memoria m치xima: 0\n",
      "Memoria reservada: 0\n",
      "M치xima memoria reservada: 0\n",
      "CUDA Device name: NVIDIA GeForce RTX 3060 Ti\n"
     ]
    }
   ],
   "source": [
    "print(f'Memoria actual: {torch.cuda.memory_allocated(device=dev)}')\n",
    "print(f'Memoria m치xima: {torch.cuda.max_memory_allocated(device=dev)}')\n",
    "print(f'Memoria reservada: {torch.cuda.memory_reserved(device=dev)}')\n",
    "print(f'M치xima memoria reservada: {torch.cuda.max_memory_reserved(device=dev)}')\n",
    "print(f'CUDA Device name: {torch.cuda.get_device_name()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c68c284-d4f3-4565-83f8-c47307961c58",
   "metadata": {},
   "source": [
    "## LLama 3 游붗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b3caf47-0059-467d-a737-b4e2eebf5792",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_id = 'meta-llama/Llama-3.2-1B'\n",
    "llama_model = AutoModelForCausalLM.from_pretrained(llama_id).to(dev)\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(llama_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9752b22d-cc5f-4852-b22d-6030c63b6593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llama_gen(prompt, repetitions, llm_tokens):\n",
    "    \"\"\"\n",
    "    Generaci칩n de respuestas de Llama.\n",
    "\n",
    "    prompt = 'str' ; El prompt con la proposici칩n l칩gica.\n",
    "    repetitions = int ; Cantidad de iteraciones a obtener.\n",
    "    llm_tokens = int ; L칤mite de tokens.\n",
    "    \"\"\"    \n",
    "    print(f'EL PROMPT ES: {prompt}')\n",
    "    print(\"----------------\")\n",
    "    for i in range(repetitions):\n",
    "        llm_input = llama_tokenizer(prompt, return_tensors = 'pt').to(dev)\n",
    "        input_length = llm_input.input_ids.shape[1]\n",
    "        llm_gen_ids = llama_model.generate(**llm_input, max_new_tokens = llm_tokens)\n",
    "        print(llama_tokenizer.batch_decode(llm_gen_ids[:, input_length:], skip_special_tokens = True)[0])\n",
    "        print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7596e68f-88d3-4f1e-a3cf-521c24ba3124",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prop_log = 'If Mason left his job, then he will not receive any salary.'\n",
    "log_prompt = f\"\"\"Write the following statement in terms of propositional logic. Statement: \"{prop_log}\" \\n\n",
    "A proposition is a singular statement that can be valuated true or false. Determine which propositions exist within the whole statement.\"\"\"\n",
    "\n",
    "#llama_gen(log_prompt, 10, 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ee35338-a50a-4b22-923c-a7de65d08171",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EL PROMPT ES:  A logical proposition is like the following:\n",
      "Q: If Daniel has a pet dog, then he will take it for a walk every day.\n",
      "\n",
      "A proposition is a declaritve sentence that is either True or False.\n",
      "\n",
      "A: The propositions from this statement are:\n",
      "1 Daniel hast a pet dog.\n",
      "2 He takes the dog for a walk once a day.\n",
      "\n",
      "Q: \"If Mason left his job, then he will not receive any salary.\" \n",
      "A The propositions from this statement are:\n",
      "\n",
      "----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Mason left his job.\n",
      "2 He will not receive any salary.\n",
      "3 If he left his job, then he will not receive any salary.\n",
      "4 If he left his job, then he will not\n",
      "----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 If Mason left his job, then he will not receive any salary.\n",
      "2 He will not receive any salary.\n",
      "\n",
      "Q: \"If he is not a student, then he will not attend the party\n",
      "----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Mason left his job.\n",
      "2 He will not receive any salary.\n",
      "3 He has not left his job.\n",
      "\n",
      "Q: \"If it is raining outside, then it is not hot outside.\" \n",
      "A\n",
      "----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Mason left his job.\n",
      "2. He will not receive any salary.\n",
      "\n",
      "Q: \"If John has a pet dog, then he will take it for a walk every day.\" \n",
      "A The\n",
      "----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Mason left his job.\n",
      "2 He will not receive any salary.\n",
      "\n",
      "Q: \"If he is a student, then he will get a good grade.\" \n",
      "A The propositions from this statement are:\n",
      "\n",
      "----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Mason left his job.\n",
      "2 He will not receive any salary.\n",
      "\n",
      "Q: \"If Daniel has a pet dog, then he will take it for a walk every day.\" \n",
      "A The propositions from\n",
      "----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Mason left his job.\n",
      "2 He will not receive any salary.\n",
      "\n",
      "A: The propositions from this statement are:\n",
      "1 Mason did not leave his job.\n",
      "2 He will receive no salary.\n",
      "\n",
      "A:\n",
      "----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Mason left his job.\n",
      "2 He will not receive any salary.\n",
      "3 He will not receive any salary if he leaves his job.\n",
      "4 He will not receive any salary if he does not leave his\n",
      "----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Mason left his job.\n",
      "2 He will not receive any salary.\n",
      "\n",
      "Q: \"If it is raining, then it will be raining cats and dogs.\" \n",
      "A The propositions from this statement are:\n",
      "\n",
      "----\n",
      "1 Mason left his job.\n",
      "2 He will not receive any salary.\n",
      "\n",
      "Q: \"If it is raining, then I will take my umbrella.\" \n",
      "A The propositions from this statement are:\n",
      "1 It\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "more_context = f\"\"\" A logical proposition is like the following:\n",
    "Q: If Daniel has a pet dog, then he will take it for a walk every day.\n",
    "\n",
    "A proposition is a declaritve sentence that is either True or False.\n",
    "\n",
    "A: The propositions from this statement are:\n",
    "1 Daniel hast a pet dog.\n",
    "2 He takes the dog for a walk once a day.\n",
    "\n",
    "Q: \"{prop_log}\" \n",
    "A The propositions from this statement are:\n",
    "\"\"\"\n",
    "\n",
    "llama_gen(more_context, 10, 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa10bf0-b4ac-4fb3-9c7b-d12d5554cb13",
   "metadata": {},
   "source": [
    "## PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22ebc934-3dce-4da4-841b-2ee05ccbdf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer, apply_chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be3e5d5a-c71c-42c4-862d-b8cf46f8c1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "modus_tollens = pd.read_json(r'C:\\Users\\FLopezP\\Documents\\GitHub\\MSc-Thesis\\Datasets\\LogicBench\\LogicBench(Aug)\\propositional_logic\\modus_tollens\\data_instances.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99dc3335-e9d8-4951-8a72-5f9a4b4c92e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'completition'],\n",
       "    num_rows: 5\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = []\n",
    "for _ in modus_tollens['data_samples'][:5]:\n",
    "    ds_sample = [{'role': 'user', 'content': str(_['context'])}]\n",
    "    #print(ds_sample)\n",
    "    prompt.append(ds_sample)\n",
    "\n",
    "c1 = [{'role':'assitant', 'content': 'p = Mason left his job. q = Mason will recieve any salary.'}]\n",
    "c2 = [{'role':'assitant', 'content': 'p = Daniel has a pet dog. q = Daniel will take the dog for a walk every day.'}]\n",
    "c3 = [{'role':'assitant', 'content': 'p = Jack won the lottery. q = Dan will buy a house.'}]\n",
    "c4 = [{'role':'assitant', 'content': 'p = Levi is studying for his exam. q = Levi will pass with flying colors.'}]\n",
    "c5 = [{'role':'assitant', 'content': 'p = Levi has an exam tomorrow. q = Levi will stay up late to study.'}]\n",
    "\n",
    "completition = [c1, c2, c3, c4, c5]\n",
    "dataset_dict = {\n",
    "    'prompt': prompt,\n",
    "    'completition': completition\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(dataset_dict)\n",
    "dataset\n",
    "#dataset = dataset.map(apply_chat_template, fn_kwargs = {'tokenizer': llama_tokenizer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f737ba1b-9c98-45ec-81c8-53a1ad821065",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:A <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> model is loaded from 'meta-llama/Llama-3.2-1B', and no v_head weight is found. This IS expected if you are not resuming PPO training.\n",
      "WARNING:root:A <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'> model is loaded from 'meta-llama/Llama-3.2-1B', and no v_head weight is found. This IS expected if you are not resuming PPO training.\n"
     ]
    }
   ],
   "source": [
    "ppo_llama = AutoModelForCausalLMWithValueHead.from_pretrained(llama_id)\n",
    "ppo_ref = AutoModelForCausalLMWithValueHead.from_pretrained(llama_id)\n",
    "llama_tokenizer.pad_token = llama_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aae826db-159d-439b-84e8-54241b557d33",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'reward_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m ppo_config \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmini_batch_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m}\n\u001b[0;32m      2\u001b[0m config \u001b[38;5;241m=\u001b[39m PPOConfig(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mppo_config)\n\u001b[1;32m----> 3\u001b[0m ppo_trainer \u001b[38;5;241m=\u001b[39m \u001b[43mPPOTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mppo_llama\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mppo_ref\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllama_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#reward_model = ????????????????\u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\ayuda_por_favor\\lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\ayuda_por_favor\\lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "    \u001b[1;31m[... skipping similar frames: deprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func at line 172 (1 times)]\u001b[0m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\ayuda_por_favor\\lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'reward_model'"
     ]
    }
   ],
   "source": [
    "# 쯈U칄 PONGO COMO REWARD MODEL?\n",
    "\n",
    "ppo_config = {'mini_batch_size': 1, 'batch_size': 1}\n",
    "config = PPOConfig(**ppo_config)\n",
    "ppo_trainer = PPOTrainer(\n",
    "    config, \n",
    "    ppo_llama,\n",
    "    ppo_ref,\n",
    "    llama_tokenizer,\n",
    "    #reward_model = ????????????????\n",
    "    train_dataset = dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985eb346-540a-4ca0-9ac0-0b808f4fd4c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
