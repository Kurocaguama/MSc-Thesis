{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acc2dbcb-eba3-4168-ad6f-c8b1ad9b76e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, GenerationConfig\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "dev = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "kuro_ds = load_dataset('Kurosawama/beam_search_DPO', split = 'train')\n",
    "folio = load_dataset('yale-nlp/FOLIO', split='validation')\n",
    "print(dev)\n",
    "\n",
    "def initialize_model(model_id, llama3):\n",
    "    quant_config = BitsAndBytesConfig(load_in_4bit = True, bnb_4bit_compute_dtype = torch.bfloat16)\n",
    "    gen_config = GenerationConfig.from_pretrained(model_id)\n",
    "    if llama3:\n",
    "        tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-3.2-1B')\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-hf')\n",
    "    tokenizer.chat_template = \"\"\"\n",
    "            <|im_start|>system\n",
    "            {SYSTEM}<|im_end|>\n",
    "            <|im_start|>user\n",
    "            {INPUT}<|im_ed|>\n",
    "            <|im_start|>assistant\n",
    "            {OUTPUT}<|im_end|>\n",
    "        \"\"\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config = quant_config, generation_config = gen_config).to(dev)\n",
    "    model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce472429-4452-4b35-979b-f9ccfed87d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34f32457e3f24e3394553aa347fb7d72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ed0320841cf4b338346f42ab7bfbb24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "419439bf394740569c7981918a6395a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacity of 3.00 GiB of which 601.81 MiB is free. Of the allocated memory 1.64 GiB is allocated by PyTorch, and 40.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m base_model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmeta-llama/Llama-2-7b-hf\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      2\u001b[0m align_model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKurosawama/Llama-2-7b-DPO-beamsearch-align\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 4\u001b[0m base_model, base_tokenizer \u001b[38;5;241m=\u001b[39m initialize_model(base_model_id, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m align_model, align_tokenizer \u001b[38;5;241m=\u001b[39m initialize_model(align_model_id, \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[2], line 26\u001b[0m, in \u001b[0;36minitialize_model\u001b[1;34m(model_id, llama3)\u001b[0m\n\u001b[0;32m     17\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmeta-llama/Llama-2-7b-hf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     18\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mchat_template \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124m        <|im_start|>system\u001b[39m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;124m        \u001b[39m\u001b[38;5;132;01m{SYSTEM}\u001b[39;00m\u001b[38;5;124m<|im_end|>\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;124m        \u001b[39m\u001b[38;5;132;01m{OUTPUT}\u001b[39;00m\u001b[38;5;124m<|im_end|>\u001b[39m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m---> 26\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id, quantization_config \u001b[38;5;241m=\u001b[39m quant_config, generation_config \u001b[38;5;241m=\u001b[39m gen_config)\u001b[38;5;241m.\u001b[39mto(dev)\n\u001b[0;32m     27\u001b[0m model\u001b[38;5;241m.\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mpad_token_id \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mpad_token_id\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model, tokenizer\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pcic\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:564\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    563\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    565\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    566\u001b[0m     )\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    570\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pcic\\Lib\\site-packages\\transformers\\modeling_utils.py:4264\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   4254\u001b[0m         load_contexts\u001b[38;5;241m.\u001b[39mappend(tp_device)\n\u001b[0;32m   4256\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(load_contexts):\n\u001b[0;32m   4257\u001b[0m         (\n\u001b[0;32m   4258\u001b[0m             model,\n\u001b[0;32m   4259\u001b[0m             missing_keys,\n\u001b[0;32m   4260\u001b[0m             unexpected_keys,\n\u001b[0;32m   4261\u001b[0m             mismatched_keys,\n\u001b[0;32m   4262\u001b[0m             offload_index,\n\u001b[0;32m   4263\u001b[0m             error_msgs,\n\u001b[1;32m-> 4264\u001b[0m         ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_load_pretrained_model(\n\u001b[0;32m   4265\u001b[0m             model,\n\u001b[0;32m   4266\u001b[0m             state_dict,\n\u001b[0;32m   4267\u001b[0m             loaded_state_dict_keys,  \u001b[38;5;66;03m# XXX: rename?\u001b[39;00m\n\u001b[0;32m   4268\u001b[0m             resolved_archive_file,\n\u001b[0;32m   4269\u001b[0m             pretrained_model_name_or_path,\n\u001b[0;32m   4270\u001b[0m             ignore_mismatched_sizes\u001b[38;5;241m=\u001b[39mignore_mismatched_sizes,\n\u001b[0;32m   4271\u001b[0m             sharded_metadata\u001b[38;5;241m=\u001b[39msharded_metadata,\n\u001b[0;32m   4272\u001b[0m             _fast_init\u001b[38;5;241m=\u001b[39m_fast_init,\n\u001b[0;32m   4273\u001b[0m             low_cpu_mem_usage\u001b[38;5;241m=\u001b[39mlow_cpu_mem_usage,\n\u001b[0;32m   4274\u001b[0m             device_map\u001b[38;5;241m=\u001b[39mdevice_map,\n\u001b[0;32m   4275\u001b[0m             offload_folder\u001b[38;5;241m=\u001b[39moffload_folder,\n\u001b[0;32m   4276\u001b[0m             offload_state_dict\u001b[38;5;241m=\u001b[39moffload_state_dict,\n\u001b[0;32m   4277\u001b[0m             dtype\u001b[38;5;241m=\u001b[39mtorch_dtype,\n\u001b[0;32m   4278\u001b[0m             hf_quantizer\u001b[38;5;241m=\u001b[39mhf_quantizer,\n\u001b[0;32m   4279\u001b[0m             keep_in_fp32_modules\u001b[38;5;241m=\u001b[39mkeep_in_fp32_modules,\n\u001b[0;32m   4280\u001b[0m             gguf_path\u001b[38;5;241m=\u001b[39mgguf_path,\n\u001b[0;32m   4281\u001b[0m             weights_only\u001b[38;5;241m=\u001b[39mweights_only,\n\u001b[0;32m   4282\u001b[0m         )\n\u001b[0;32m   4284\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[0;32m   4285\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pcic\\Lib\\site-packages\\transformers\\modeling_utils.py:4777\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[1;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules, gguf_path, weights_only)\u001b[0m\n\u001b[0;32m   4773\u001b[0m                 set_module_tensor_to_device(\n\u001b[0;32m   4774\u001b[0m                     model_to_load, key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;241m*\u001b[39mparam\u001b[38;5;241m.\u001b[39msize(), dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m   4775\u001b[0m                 )\n\u001b[0;32m   4776\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4777\u001b[0m         new_error_msgs, offload_index, state_dict_index \u001b[38;5;241m=\u001b[39m _load_state_dict_into_meta_model(\n\u001b[0;32m   4778\u001b[0m             model_to_load,\n\u001b[0;32m   4779\u001b[0m             state_dict,\n\u001b[0;32m   4780\u001b[0m             start_prefix,\n\u001b[0;32m   4781\u001b[0m             expected_keys,\n\u001b[0;32m   4782\u001b[0m             device_map\u001b[38;5;241m=\u001b[39mdevice_map,\n\u001b[0;32m   4783\u001b[0m             offload_folder\u001b[38;5;241m=\u001b[39moffload_folder,\n\u001b[0;32m   4784\u001b[0m             offload_index\u001b[38;5;241m=\u001b[39moffload_index,\n\u001b[0;32m   4785\u001b[0m             state_dict_folder\u001b[38;5;241m=\u001b[39mstate_dict_folder,\n\u001b[0;32m   4786\u001b[0m             state_dict_index\u001b[38;5;241m=\u001b[39mstate_dict_index,\n\u001b[0;32m   4787\u001b[0m             dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m   4788\u001b[0m             hf_quantizer\u001b[38;5;241m=\u001b[39mhf_quantizer,\n\u001b[0;32m   4789\u001b[0m             is_safetensors\u001b[38;5;241m=\u001b[39mis_safetensors,\n\u001b[0;32m   4790\u001b[0m             keep_in_fp32_modules\u001b[38;5;241m=\u001b[39mkeep_in_fp32_modules,\n\u001b[0;32m   4791\u001b[0m             unexpected_keys\u001b[38;5;241m=\u001b[39munexpected_keys,\n\u001b[0;32m   4792\u001b[0m         )\n\u001b[0;32m   4793\u001b[0m         error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_error_msgs\n\u001b[0;32m   4794\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4795\u001b[0m     \u001b[38;5;66;03m# Sharded checkpoint or whole but low_cpu_mem_usage==True\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pcic\\Lib\\site-packages\\transformers\\modeling_utils.py:944\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[1;34m(model, state_dict, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys, pretrained_model_name_or_path)\u001b[0m\n\u001b[0;32m    942\u001b[0m     set_module_tensor_to_device(model, param_name, param_device, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mset_module_kwargs)\n\u001b[0;32m    943\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 944\u001b[0m     hf_quantizer\u001b[38;5;241m.\u001b[39mcreate_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)\n\u001b[0;32m    945\u001b[0m     \u001b[38;5;66;03m# For quantized modules with FSDP/DeepSpeed Stage 3, we need to quantize the parameter on the GPU\u001b[39;00m\n\u001b[0;32m    946\u001b[0m     \u001b[38;5;66;03m# and then cast it to CPU to avoid excessive memory usage on each GPU\u001b[39;00m\n\u001b[0;32m    947\u001b[0m     \u001b[38;5;66;03m# in comparison to the sharded model across GPUs.\u001b[39;00m\n\u001b[0;32m    948\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_fsdp_enabled() \u001b[38;5;129;01mor\u001b[39;00m is_deepspeed_zero3_enabled():\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pcic\\Lib\\site-packages\\transformers\\quantizers\\quantizer_bnb_4bit.py:238\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.create_quantized_param\u001b[1;34m(self, model, param_value, param_name, target_device, state_dict, unexpected_keys)\u001b[0m\n\u001b[0;32m    235\u001b[0m         new_value \u001b[38;5;241m=\u001b[39m new_value\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m    237\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m old_value\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\n\u001b[1;32m--> 238\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m bnb\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mParams4bit(new_value, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\u001b[38;5;241m.\u001b[39mto(target_device)\n\u001b[0;32m    240\u001b[0m module\u001b[38;5;241m.\u001b[39m_parameters[tensor_name] \u001b[38;5;241m=\u001b[39m new_value\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pcic\\Lib\\site-packages\\bitsandbytes\\nn\\modules.py:331\u001b[0m, in \u001b[0;36mParams4bit.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    328\u001b[0m device, dtype, non_blocking, convert_to_format \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39m_parse_to(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m device\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbnb_quantized:\n\u001b[1;32m--> 331\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_quantize(device)\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    333\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant_state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pcic\\Lib\\site-packages\\bitsandbytes\\nn\\modules.py:295\u001b[0m, in \u001b[0;36mParams4bit._quantize\u001b[1;34m(self, device)\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_quantize\u001b[39m(\u001b[38;5;28mself\u001b[39m, device):\n\u001b[1;32m--> 295\u001b[0m     w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    296\u001b[0m     w_4bit, quant_state \u001b[38;5;241m=\u001b[39m bnb\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mquantize_4bit(\n\u001b[0;32m    297\u001b[0m         w,\n\u001b[0;32m    298\u001b[0m         blocksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocksize,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    301\u001b[0m         quant_storage\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant_storage,\n\u001b[0;32m    302\u001b[0m     )\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m w_4bit\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 86.00 MiB. GPU 0 has a total capacity of 3.00 GiB of which 601.81 MiB is free. Of the allocated memory 1.64 GiB is allocated by PyTorch, and 40.93 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "base_model_id = 'meta-llama/Llama-2-7b-hf'\n",
    "align_model_id = 'Kurosawama/Llama-2-7b-DPO-beamsearch-align'\n",
    "\n",
    "base_model, base_tokenizer = initialize_model(base_model_id, False)\n",
    "align_model, align_tokenizer = initialize_model(align_model_id, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70bbdef9-7baa-4850-9e6f-b2a6ed0bdd7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['story_id', 'premises', 'premises-FOL', 'conclusion', 'conclusion-FOL', 'label', 'example_id'],\n",
       "    num_rows: 203\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e15a5480-74eb-4764-bfd3-64ace337bf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer(prompt, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Genera la respuesta del modelo siguiendo la estrategia de beam_search.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors = 'pt').to(dev)\n",
    "    outputs = model.generate(**inputs, max_new_tokens = 200, num_beams = 3)\n",
    "    ans = tokenizer.batch_decode(outputs, skip_special_tokens = True)[0]\n",
    "    ans = ans[len(prompt):]\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c80bd49c-ce59-4bbc-98be-77f3fba1515c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_ex = \"\"\"\n",
    "Given a problem description and a question. The task is to parse the problem and the question into first-order logic formulars.\n",
    "    The grammar of the first-order logic formular is defined as follows:\n",
    "    1) logical conjunction of expr1 and expr2: expr1 ∧ expr2\n",
    "    2) logical disjunction of expr1 and expr2: expr1 ∨ expr2\n",
    "    3) logical exclusive disjunction of expr1 and expr2: expr1 ⊕ expr2\n",
    "    4) logical negation of expr1: ¬expr1\n",
    "    5) expr1 implies expr2: expr1 → expr2\n",
    "    6) expr1 if and only if expr2: expr1 ↔ expr2\n",
    "    7) logical universal quantification: ∀x\n",
    "    8) logical existential quantification: ∃x\n",
    "    --------------\n",
    "    Problem:\n",
    "    All people who regularly drink coffee are dependent on caffeine. People either regularly drink coffee or joke about being addicted to caffeine. No one who jokes about being addicted to caffeine is unaware that caffeine is a drug. Rina is either a student and unaware that caffeine is a drug, or neither a student nor unaware that caffeine is a drug. If Rina is not a person dependent on caffeine and a student, then Rina is either a person dependent on caffeine and a student, or neither a person dependent on caffeine nor a student.\n",
    "    Predicates:\n",
    "    Dependent(x) ::: x is a person dependent on caffeine.\n",
    "    Drinks(x) ::: x regularly drinks coffee.\n",
    "    Jokes(x) ::: x jokes about being addicted to caffeine.\n",
    "    Unaware(x) ::: x is unaware that caffeine is a drug.\n",
    "    Student(x) ::: x is a student.\n",
    "    Premises:\n",
    "    ∀x (Drinks(x) → Dependent(x)) ::: All people who regularly drink coffee are dependent on caffeine.\n",
    "    ∀x (Drinks(x) ⊕ Jokes(x)) ::: People either regularly drink coffee or joke about being addicted to caffeine.\n",
    "    ∀x (Jokes(x) → ¬Unaware(x)) ::: No one who jokes about being addicted to caffeine is unaware that caffeine is a drug. \n",
    "    (Student(rina) ∧ Unaware(rina)) ⊕ ¬(Student(rina) ∨ Unaware(rina)) ::: Rina is either a student and unaware that caffeine is a drug, or neither a student nor unaware that caffeine is a drug. \n",
    "    ¬(Dependent(rina) ∧ Student(rina)) → (Dependent(rina) ∧ Student(rina)) ⊕ ¬(Dependent(rina) ∨ Student(rina)) ::: If Rina is not a person dependent on caffeine and a student, then Rina is either a person dependent on caffeine and a student, or neither a person dependent on caffeine nor a student.\n",
    "    --------------\n",
    "    \n",
    "    Problem:\n",
    "    {}\n",
    "    Predicates:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "782ffe3d-db37-4761-a410-f4955f14898d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "People in this club who perform in school talent shows often attend and are very engaged with school events.\n",
      "People in this club either perform in school talent shows often or are inactive and disinterested community members.\n",
      "People in this club who chaperone high school dances are not students who attend the school.\n",
      "All people in this club who are inactive and disinterested members of their community chaperone high school dances.\n",
      "All young children and teenagers in this club who wish to further their academic careers and educational opportunities are students who attend the school. \n",
      "Bonnie is in this club and she either both attends and is very engaged with school events and is a student who attends the school or is not someone who both attends and is very engaged with school events and is not a student who attends the school. \n",
      " ∀x (InThisClub(x) ∧ PerformOftenIn(x, schoolTalentShow) → Attend(x, schoolEvent) ∧ VeryEngagedWith(x, schoolEvent))\n",
      "∀x (InThisClub(x) → PerformOftenIn(x, schoolTalentShow) ⊕ (InActive(x) ∧ Disinterested(x) ∧ MemberOf(x, community)))\n",
      "∀x (InThisClub(x) ∧ Chaperone(x, highSchoolDance) → ¬(Studen(x) ∧ AttendSchool(x)))\n",
      "∀x (InThisClub(x) ∧ (InActive(x) ∧ Disinterested(x) ∧ MemberOf(x, community)) → Chaperone(x, highSchoolDances))\n",
      "∀x (InThisClub(x) ∧ (YoungChildren(x) ⊕ Teenager(x)) ∧ WishToFurther(x, academicCareer)) → Studen(x) ∧ AttendSchool(x))\n",
      "InThisClub(bonnie) ∧ ¬((Attend(x, schoolEvent) ∧ VeryEngagedWith(bonnie, schoolEvent)) ⊕ (Studen(bonne) ∧ AttendSchool(bonnie)))\n"
     ]
    }
   ],
   "source": [
    "print(folio['premises'][1], '\\n', folio['premises-FOL'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77de09f3-7c58-4fde-9712-6d4ab4ffe596",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(answer(prompt_ex.format(folio['premises'][1]), base_model, base_tokenizer))\n",
    "print(\"---------------------\")\n",
    "print(answer(prompt_ex.format(folio['premises'][1]), align_model, align_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e122fd9e-ff07-4b30-bd4a-fe80497e2c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# Lo que sigue es una forma de automatizar la evaluación y poder medir valores como Accuracy y demás.\n",
    "# No obstante la evaluación está inconclusa y no se usa para el proyecto de RL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e03b6edf-86e3-43d2-8de0-e70a9cdd58e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "∀x (DrinkRegularly(x, coffee) → IsDependentOn(x, caffeine)) ∀x (DrinkRegularly(x, coffee)  ∨ (¬WantToBeAddictedTo(x, caffeine))) ∀x (¬WantToBeAddictedTo(x, caffeine) → ¬AwareThatDrug(x, caffeine)) ¬(Student(rina) ⊕  ¬AwareThatDrug(rina, caffeine)) ¬(IsDependentOn(rina, caffeine) ⊕ Student(rina))  \n",
      " --- \n",
      "  Dependent(x) ::: x is a person dependent on caffeine.\n",
      "             Drinks(x) ::: x regularly drinks coffee.\n",
      "             Jokes(x) ::: x jokes about being addicted to caffeine.\n",
      "             Unaware(x) ::: x is unaware that caffeine is a drug.\n",
      "             Student(x) ::: x is a student.\n",
      "             Premises:\n",
      "             ∀x (Drinks(x) → Dependent(x)) ::: All people who regularly drink coffee are dependent on caffeine.\n",
      "             ∀x (Drinks(x) ⊕ Jokes(x)) ::: People who either regularly drink coffee or joke about being addicted to caffeine.\n",
      "             ∀x (Jokes(x) → ¬Unaware(x)) ::: No one who jokes about being addicted to caffeine is unaware that caffeine is a drug. \n",
      "             (Student(rina) ∧ Unaware(rina)) ⊕ ¬(Student(rina) ∨ Unaware(rina)) ::: Rina is either a student and unaware that caffeine is a drug, or neither a student nor unaware\n"
     ]
    }
   ],
   "source": [
    "def get_answer(dataset, ans_num):\n",
    "    \"\"\"\n",
    "    dataset = huggingface dataset \n",
    "    ans_num = int ; Valor de la entrada del dataset que queremos analizar\n",
    "\n",
    "    Nos regresa las respuestas del modelo filtradas. \n",
    "    \"\"\"\n",
    "    chosen = dataset['chosen'][ans_num][1]['content']\n",
    "    rejected = dataset['rejected'][ans_num][1]['content']\n",
    "    return chosen, rejected\n",
    "\n",
    "cho1, rej1 = get_answer(kuro_ds, 0)\n",
    "print(cho1, '\\n', '---', '\\n', rej1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e17a3a97-c2ff-42a3-9062-a4d2d5135c32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'∀x (DrinkRegularly(x, coffee) → IsDependentOn(x, caffeine)) ∀x (DrinkRegularly(x, coffee)  ∨ (¬WantToBeAddictedTo(x, caffeine))) ∀x (¬WantToBeAddictedTo(x, caffeine) → ¬AwareThatDrug(x, caffeine)) ¬(Student(rina) ⊕  ¬AwareThatDrug(rina, caffeine)) ¬(IsDependentOn(rina, caffeine) ⊕ Student(rina)) '"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cho1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6b8fea72-2042-4326-af7b-d692500fb714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '(DrinkRegularly(x, coffee) → IsDependentOn(x, caffeine)) ',\n",
       " '(DrinkRegularly(x, coffee)  ∨ (¬WantToBeAddictedTo(x, caffeine))) ',\n",
       " '(¬WantToBeAddictedTo(x, caffeine) → ¬AwareThatDrug(x, caffeine)) ¬(Student(rina) ⊕  ¬AwareThatDrug(rina, caffeine)) ¬(IsDependentOn(rina, caffeine) ⊕ Student(rina)) ']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# '∀x|∃x'\n",
    "cho1.split('∀x ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "82011c54-7dbc-4b79-8251-8b1af036adf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " ' (DrinkRegularly(x, coffee) → IsDependentOn(x, caffeine)) ',\n",
       " ' (DrinkRegularly(x, coffee)  ∨ (¬WantToBeAddictedTo(x, caffeine))) ',\n",
       " ' (¬WantToBeAddictedTo(x, caffeine) → ¬AwareThatDrug(x, caffeine)) ',\n",
       " 'Student(rina) ⊕  ¬AwareThatDrug(rina, caffeine)) ',\n",
       " 'IsDependentOn(rina, caffeine) ⊕ Student(rina)) ']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "re.split('∀x|∃x|¬\\(', cho1) # Casi pero no nos regesa la ER."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002a17a6-41d2-4417-a935-90b25836b9f2",
   "metadata": {},
   "source": [
    "**VER QUÉ VERGAS VAMOS A HACER PARA LA EVALUACIÓN Y LA SEPARACIÓN DE PREMISAS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aeeec4c-2cc5-4120-ac91-8730a9191578",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
