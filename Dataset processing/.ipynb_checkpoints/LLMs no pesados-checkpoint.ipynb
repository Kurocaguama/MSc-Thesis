{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88d16282-3bed-4fbc-b97e-c972331cd701",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers, torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90ea6339-0370-47bb-a2a0-a049cf82e385",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       0    |       0    |       0    |       0    |\n",
      "|       from large pool |       0    |       0    |       0    |       0    |\n",
      "|       from small pool |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dev = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(dev)\n",
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14dd0057-644b-49b3-bd33-fb33008d8425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memoria actual: 4951781888\n",
      "Memoria m치xima: 4976395776\n",
      "Memoria reservada: 5007998976\n",
      "M치xima memoria reservada: 5007998976\n",
      "CUDA Device name: NVIDIA GeForce RTX 3060 Ti\n"
     ]
    }
   ],
   "source": [
    "print(f'Memoria actual: {torch.cuda.memory_allocated(device=dev)}')\n",
    "print(f'Memoria m치xima: {torch.cuda.max_memory_allocated(device=dev)}')\n",
    "print(f'Memoria reservada: {torch.cuda.memory_reserved(device=dev)}')\n",
    "print(f'M치xima memoria reservada: {torch.cuda.max_memory_reserved(device=dev)}')\n",
    "print(f'CUDA Device name: {torch.cuda.get_device_name()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c68c284-d4f3-4565-83f8-c47307961c58",
   "metadata": {},
   "source": [
    "## LLama 3 游붗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b3caf47-0059-467d-a737-b4e2eebf5792",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_id = 'meta-llama/Llama-3.2-1B'\n",
    "llama_model = AutoModelForCausalLM.from_pretrained(llama_id).to(dev)\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(llama_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7596e68f-88d3-4f1e-a3cf-521c24ba3124",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EL PROMPT ES: Write the following statement in terms of propositional logic: \"If Mason left his job, then he will not receive any salary.\" Determine which propositions exist.\n",
      "----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (a) Mason left his job. (b) Mason will not receive any salary. (c) Mason received his salary.\n",
      "Write the following statement in terms of propositional logic: \"If Mason left his job, then he will not receive any salary.\" Determine which propositions exist. (a) Mason left his job. (b) Mason will not receive any salary.\n",
      "----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Write the statement in terms of these propositions.\n",
      "Question: Write the following statement in terms of propositional logic: \"If Mason left his job, then he will not receive any salary.\" Determine which propositions exist. Write the statement in terms of these propositions.\n",
      "Question: Write the following statement in terms of propositional logic: \"If Mason left his job, then he will not\n",
      "----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " What are the logical equivalences between these propositions? Write the statement in the form of a conditional statement.\n",
      "A. If Mason left his job, then he will not receive any salary.\n",
      "----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Determine which propositions are not true. Determine which propositions are true.\n",
      "The proposition is true because if Mason leaves his job, he will not receive any salary. Therefore, it is true.\n",
      "----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1. If Mason left his job, then he will receive no salary. 2. If Mason left his job, then he will receive a salary. 3. If Mason left his job, then he will not receive a salary. 4. If Mason left his job, then he will receive no salary or a salary. 5. If Mason left his\n",
      "----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Write the statement in symbolic form.\n",
      "If Mason left his job, then he will not receive any salary.\n",
      "A. p and not q\n",
      "B. p or not q\n",
      "C. p and q\n",
      "D. p and not q\n",
      "E. p or not q\n",
      "Answer: A\n",
      "----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Determine which propositions are necessary to be true for the statement to be true. Determine which propositions are sufficient to be true for the statement to be true. Determine which propositions are necessary to be false for the statement to be false. Determine which propositions are sufficient to be false for the statement to be false.\n",
      "----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (1 point) Mason will not receive any salary. Mason will receive a salary. Mason has left his job. Mason has not left his job.\n",
      "Write the following statement in terms of propositional logic: \"If Mason left his job, then he will not receive any salary.\" Determine which propositions exist. (1 point) Mason will not receive any salary. Mason will receive\n",
      "----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Write the following statement in terms of propositional logic: \"If Mason left his job, then he will not receive any salary.\" Determine which propositions exist.\n",
      "Write the following statement in terms of propositional logic: \"If Mason left his job, then he will not receive any salary.\" Determine which propositions exist.\n",
      "Write the following statement in terms of propositional logic: \"If\n",
      "----\n",
      " Write the following statement in terms of propositional logic: \"If Mason left his job, then he will not receive any salary.\" Determine which propositions exist.\n",
      "The statement is in the form of a conditional statement. The subject is the one on the left side of the \"if\" and the predicate is the one on the right side of the \"if\". In the statement,\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "prop_log = 'If Mason left his job, then he will not receive any salary.'\n",
    "log_prompt = f'Write the following statement in terms of propositional logic: \"{prop_log}\" Determine which propositions exist.'\n",
    "\n",
    "llama_gen(log_prompt, 10, 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9752b22d-cc5f-4852-b22d-6030c63b6593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llama_gen(prompt, repetitions, llm_tokens):\n",
    "    \"\"\"\n",
    "    Generaci칩n de respuestas de Llama.\n",
    "\n",
    "    prompt = 'str' ; El prompt con la proposici칩n l칩gica.\n",
    "    repetitions = int ; Cantidad de iteraciones a obtener.\n",
    "    llm_tokens = int ; L칤mite de tokens.\n",
    "    \"\"\"    \n",
    "    print(f'EL PROMPT ES: {prompt}')\n",
    "    print(\"----------------\")\n",
    "    for i in range(repetitions):\n",
    "        llm_input = llama_tokenizer(prompt, return_tensors = 'pt').to(dev)\n",
    "        input_length = llm_input.input_ids.shape[1]\n",
    "        llm_gen_ids = llama_model.generate(**llm_input, max_new_tokens = llm_tokens)\n",
    "        print(llama_tokenizer.batch_decode(llm_gen_ids[:, input_length:], skip_special_tokens = True)[0])\n",
    "        print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee35338-a50a-4b22-923c-a7de65d08171",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
